{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting body text\n",
    "Extract the body text and keep adding that to the corpus file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json   # to read the initial files\n",
    "import os\n",
    "import regex as re  # for pre processing\n",
    "from collections import Counter  \n",
    "from nltk import ngrams, sent_tokenize, word_tokenize\n",
    "import joblib   # saving and loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function as stated in the pdf\n",
    "def extract_body_text( filename : str ) -> str :\n",
    "\n",
    "    file = open(filename)\n",
    "    paper_content = json.load(file)\n",
    "    body_text = \" \"\n",
    "\n",
    "    if 'body_text' in paper_content :\n",
    "        for bt in paper_content['body_text']:\n",
    "            body_text = body_text + bt['text']\n",
    "    return ( body_text + '\\n').lower()\n",
    "\n",
    "\n",
    "# this function opens each json file, extracts the body text and add that text to\n",
    "# an output file. This creates a single txt file which will be our corpus.\n",
    "# We use a buffer of 200, 200 files are opened and extracted to a single variable, \n",
    "# this is added to output, and then the variable is reset.\n",
    "def create_corpus(file):\n",
    "    basepath = 'pdf_json'   # folder location\n",
    "    length = len(os.listdir(basepath))   # number of files in the folder\n",
    "    num = 200    # buffer length\n",
    "\n",
    "    with open(file,'w',encoding=\"utf-8\") as f:  # open output file\n",
    "        f.write(\"\")\n",
    "\n",
    "    for i in range(0,length//num+1):\n",
    "        corpus = \"\"   # buffer variable\n",
    "        j = length - i*num if ((i+1)*num > length) else num  \n",
    "        for entry in os.listdir(basepath)[(i*num):(j+i*num)]:\n",
    "            pth = os.path.join(basepath, entry)\n",
    "            if os.path.isfile(pth):\n",
    "                corpus += extract_body_text(pth)  # add text to buffer\n",
    "        with open(file,'a',encoding=\"utf-8\") as f:\n",
    "            f.write(corpus)  # when buffer reached, add to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_corpus(\"corpus_covid.txt\")  # creating corpus and naming it corpus_covid.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text preprocessing, we do the following :\n",
    "1. Remove brackets and the text within them. This is done by the regex rule :  \\\\([^()]\\*\\\\) | \\\\[[^\\[\\]]*\\\\]\n",
    "2. Remove all occurances of the text of the form : a1111... (these appear in some documents)\n",
    "3. Remove occurances of 'et al.' followed by reference numbers if they occur\n",
    "4. Remove occurances of these : &nbsp; j o u r n a l p r o o f &nbsp; , &nbsp; j o u r n a l p r e -p r o o f &nbsp; , &nbsp; a c c e p t e d m a n u s c r i p t\n",
    "5. Remove all numbers, and these symbols : @  #  $  \\  /  *  :  ;  %\n",
    "The above 5 are done by using a regex pattern to find them and then substitute them with whitespace\n",
    "6. Then the result is passed through a map which removes all the non ascii charecters\n",
    "7. That is then passed through .split() and then joined to remove all the unnecessary whitespace which is created while the regex substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to do the process above\n",
    "def clean(text):\n",
    "    return \" \".join( \n",
    "        ''.join(\n",
    "            map(\n",
    "                lambda x: x if ord(x)<128 else ' ', \n",
    "                re.sub(r'a1+|\\([^()]*\\)|\\[[^\\[\\]]*\\]|(et al\\. [0-9]*)|j o u r n a l p r o o f|j o u r n a l p r e -p r o o f|a c c e p t e d m a n u s c r i p t|[0-9@#\\$\\\\/\\*,:;%]*','',text)\n",
    "                )).split())\n",
    "\n",
    "# applies the function to each line of the corpus, which is a single file each\n",
    "# it saves the result with \"preprocessed_\" prefixed to the original file name\n",
    "def prepreocess_corpus(file):\n",
    "    corpus = open(file,'r',encoding=\"utf-8\")\n",
    "    preprocessed_corpus = open(\"preprocessed_\"+file,'w',encoding=\"utf-8\")\n",
    "\n",
    "    line = corpus.readline()\n",
    "\n",
    "    while True:\n",
    "        if not line:  #stop when ended\n",
    "            break\n",
    "        preprocessed_corpus.write(clean(line)+'\\n')\n",
    "        line = corpus.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepreocess_corpus(\"corpus_covid.txt\") # preprocessing the corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate all the unigrams and add those to the counter. This gives us all the words and all their counts. These can be used for vocabulary and for calculating laplace smoothing for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate all the unigrams from the file given\n",
    "def gen_unigrams(file):\n",
    "    x = Counter()   # Counter to store unigrams\n",
    "    with open(file, 'r',encoding=\"utf-8\") as f:\n",
    "        for line in f:  # going line by line\n",
    "             for sent in sent_tokenize(line): # extract sentences from each line\n",
    "                x.update([i[0] for i in ngrams(word_tokenize(sent), 1)])  # create unigrams and add them to counter\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the unigrams of the preprocessed corpus\n",
    "vocab_counts = gen_unigrams(\"preprocessed_corpus_covid.txt\")\n",
    "\n",
    "# dumping this into the uni_model file for future use\n",
    "joblib.dump(vocab_counts, './models/uni_model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to load the file\n",
    "vocab_counts = joblib.load('./models/uni_model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab is 1990740\n"
     ]
    }
   ],
   "source": [
    "# the size of vocab will be the number of entries in vocab_counts\n",
    "print(\"Size of vocab is\",len(vocab_counts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code block generates the vocab as a list and dumps it into a file. This be be used for future reference as the bigram and trigram models don't directly save the words, but instead use the position of the word in this vocab to reduce their size in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to load the unigrams\n",
    "uni = joblib.load('./models/uni_model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/vocab.pickle']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab is all the keys in above along with '<s>' and '</s>', which denote the start and stop of sentences\n",
    "vocab = list(uni.keys())\n",
    "vocab.extend(['<s>','</s>'])\n",
    "\n",
    "# save this\n",
    "joblib.dump(vocab, './models/vocab.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to load vocab\n",
    "vocab = joblib.load('./models/vocab.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an inverted list so that position of word can be obtained faster\n",
    "invert_vocab = dict((vocab[i],i) for i in range(len(vocab)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(_bigram and trigram were generated usning python scripts to speed up the process_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both, the bigram and trigram models, we load the corpus line-by-line, tokenize the sentences from that line, generate bi- or trigrams from that sentence with padded symbols and then add those grams to the counter after the words are converted to numbers using the inverted vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bigrams(file):\n",
    "    x = Counter() \n",
    "    with open(file, 'r',encoding=\"utf-8\") as f: \n",
    "        for line in f:   # reading line by line\n",
    "             for sent in sent_tokenize(line):  # extract sentences from line\n",
    "                gram = list(ngrams(word_tokenize(sent), 2,\n",
    "                                    pad_left=True, pad_right=True,\n",
    "                                    left_pad_symbol='<s>',\n",
    "                                    right_pad_symbol='</s>'))\n",
    "                x.update(list(map(lambda t: tuple(map(lambda y: invert_vocab[y], t)),gram)))  \n",
    "                # changes the list of tuples of words to list of tuples of numbers which correspond to inverted_vocab\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the bigrams and dump the counter object into bi_model.pickle\n",
    "bi = gen_bigrams(\"preprocessed_corpus_covid.txt\")\n",
    "\n",
    "filenm = \"./models/bi_model.pickle\"\n",
    "joblib.dump(bi, filenm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_trigrams(file):\n",
    "    x = Counter()\n",
    "    with open(file, 'r',encoding=\"utf-8\") as f:\n",
    "        for line in f:   # reading line by line\n",
    "             for sent in sent_tokenize(line):  # extract sentences from line\n",
    "                gram = list(ngrams(word_tokenize(sent), 3,\n",
    "                                    pad_left=True, pad_right=True,\n",
    "                                    left_pad_symbol='<s>',\n",
    "                                    right_pad_symbol='</s>'))\n",
    "                x.update(list(map(lambda t: tuple(map(lambda y: invert_vocab[y], t)),gram)))\n",
    "                # changes the list of tuples of words to list of tuples of numbers which correspond to inverted_vocab\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the bigrams and dump the counter object into tri_model.pickle\n",
    "tri = gen_trigrams(\"preprocessed_corpus_covid.txt\")\n",
    "\n",
    "filenm = \"./models/tri_model.pickle\"\n",
    "joblib.dump(tri, filenm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to load unigram model\n",
    "uni = joblib.load(\"./models/uni_model.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the bigram model\n",
    "bi = joblib.load(\"./models/bi_model.pickle\")\n",
    "\n",
    "# vocab size\n",
    "v1 = len(uni.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes bigrams in terms of numbers with the mapping in vocab \n",
    "# and returns the laplace smoothened probability of the bigram\n",
    "def bi_prob(bigram):\n",
    "    return ((bi[bigram]+1)/(uni[bigram[0]]+v1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bi_predict takes in a string of the form 'x _ y', 'x \\_' or '\\_ x', takes probability of each of them happening and returns the 10 highest ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability if the black is to the right\n",
    "def bi_predict_right(word):\n",
    "    word = invert_vocab[word]   # convert word to number\n",
    "    # Take all the bigrams which match the first word to that before blank and take their probabilities\n",
    "    # then takes the most common 10 from them\n",
    "    counts =  Counter({i[1]:bi_prob(i) for i in bi.keys() if i[0]==word}).most_common(10)\n",
    "    # converts the numbers back to word\n",
    "    return list(map(lambda t1: (vocab[t1[0]],t1[1]),counts))\n",
    "\n",
    "\n",
    "# probability if the blank is to the left\n",
    "# function same as above, matching second word to the forst word after blank\n",
    "def bi_predict_left(word):\n",
    "    word = invert_vocab[word]\n",
    "    counts =  Counter({i[0]:bi_prob(i) for i in bi.keys() if i[1]==word}).most_common(10)\n",
    "    return list(map(lambda t1: (vocab[t1[0]],t1[1]),counts))\n",
    "\n",
    "\n",
    "# takes a string, splits along with blank and check position of the blank\n",
    "# if there is only one word, we take only one probability\n",
    "# otherwise we combine both and return the highers 10\n",
    "def bi_predict(blank):\n",
    "    all=[]\n",
    "    blank = blank.split()\n",
    "    if(blank[1] == \"_\"):\n",
    "        all +=  bi_predict_right(blank[0])\n",
    "        if(len(blank)==3):\n",
    "            all += bi_predict_left(blank[2])\n",
    "    else:\n",
    "        all = bi_predict_left(blank[1])\n",
    "    all.sort(key=lambda x: x[1], reverse=True)\n",
    "    return all[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all houses were ____ ventilated\n",
      "\n",
      "('not', 0.017188080814169605)\n",
      "('used', 0.01670835970543617)\n",
      "('also', 0.01301626530837779)\n",
      "('performed', 0.012497362789716387)\n",
      "('found', 0.010994906416709364)\n",
      "('collected', 0.010508655072987934)\n",
      "('obtained', 0.008821845143012146)\n",
      "('observed', 0.008150737916553644)\n",
      "('identified', 0.007983965761475632)\n"
     ]
    }
   ],
   "source": [
    "# first sentence\n",
    "print(\"all houses were ____ ventilated\\n\", *bi_predict(\"were _ ventilated\"),sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "develop an integrated ____ to reach\n",
      "\n",
      "('due', 0.07302761787074154)\n",
      "('<s>', 0.04589298451831982)\n",
      "('used', 0.044714025940102674)\n",
      "('compared', 0.041695550398344335)\n",
      "('according', 0.03419381737444367)\n",
      "('related', 0.02958296914715131)\n",
      "('order', 0.025755749118418277)\n",
      "('able', 0.023921255412560155)\n",
      "('and', 0.02348724594874268)\n"
     ]
    }
   ],
   "source": [
    "# second sentence\n",
    "print(\"\\n\\ndevelop an integrated ____ to reach\\n\", *bi_predict(\"integrated _ to\"),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "diagnosis and treatment ____ by involved\n",
      "\n",
      "('followed', 0.02083345891477541)\n",
      "('caused', 0.020452695982398506)\n",
      "('of', 0.019566091001336185)\n",
      "('<s>', 0.012695781468197756)\n",
      "('.', 0.010257492188834302)\n",
      "('characterized', 0.009442719792639923)\n",
      "('determined', 0.009106663853642364)\n",
      "('induced', 0.008085937892442007)\n",
      "('affected', 0.007775500567628118)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\ndiagnosis and treatment ____ by involved\\n\", *bi_predict(\"treatment _ by\"),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "involving non-health ____ stakeholders from . . .\n",
      "\n",
      "('the', 6.731165295317319e-05)\n",
      "('and', 6.17860695017933e-05)\n",
      "('of', 5.726513758702794e-05)\n",
      "('a', 3.365582647658659e-05)\n",
      "('each', 3.265117493997207e-05)\n",
      "('care', 2.4111636878748605e-05)\n",
      "('to', 1.7581401890754193e-05)\n",
      "('<s>', 1.7581401890754193e-05)\n",
      "('different', 1.6576750354139668e-05)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\ninvolving non-health ____ stakeholders from . . .\\n\", *bi_predict(\"non-health _ stakeholder\"),sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above prediction, the 5th prediction seems to fit the most, but it is overshadowed by the the other more popular bigrams which occur with stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "this is because engineers do not work in ____ but rather as a team\n",
      "\n",
      "('the', 0.5839969056732672)\n",
      "('a', 0.10926640344796408)\n",
      "('this', 0.07213197102584969)\n",
      "('addition', 0.041320815375187114)\n",
      "('patients', 0.03648291590061987)\n",
      "('our', 0.02647256798979274)\n",
      "('order', 0.023836864683484532)\n",
      "('which', 0.02352391572982911)\n",
      "('vitro', 0.021041421782854616)\n"
     ]
    }
   ],
   "source": [
    "# third sentence\n",
    "print(\"\\n\\nthis is because engineers do not work in ____ but rather as a team\\n\", *bi_predict(\"in _ but\"), sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functions listed below are similar to the ones used for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri = joblib.load(\"./models/tri_model.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = joblib.load(\"./models/bi_model.pickle\")\n",
    "v2 = len(bi.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_prob(word):\n",
    "    return ((tri[word]+1)/(bi[word[0:2]]+v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if blank is to the right\n",
    "def tri_predict_right(word):\n",
    "    word = tuple(map(lambda x: invert_vocab[x], word))\n",
    "    counts =  Counter({i[2]:tri_prob(i) for i in tri.keys() if i[0:2]==word}).most_common(10)\n",
    "    return list(map(lambda t1: (vocab[t1[0]],t1[1]),counts))\n",
    "\n",
    "# if blank is in the middle\n",
    "def tri_predict_mid(word):\n",
    "    word = tuple(map(lambda x: invert_vocab[x], word))\n",
    "    counts =  Counter({i[2]:tri_prob(i) for i in tri.keys() if (i[0],i[2])==word}).most_common(10)\n",
    "    return list(map(lambda t1: (vocab[t1[0]],t1[1]),counts))\n",
    "\n",
    "# if blank is to the left\n",
    "def tri_predict_left(word):\n",
    "    word = tuple(map(lambda x: invert_vocab[x], word))\n",
    "    counts =  Counter({i[2]:tri_prob(i) for i in tri.keys() if i[1:3]==word}).most_common(10)\n",
    "    return list(map(lambda t1: (vocab[t1[0]],t1[1]),counts))\n",
    "\n",
    "# takes strings of the form \"x y _\", \"x y _ z\", \"x y _ z w\", \"y _ z w\" or \"_ z w\"\n",
    "def tri_predict(blank):\n",
    "    all=[]\n",
    "    blank = list(map(lambda x: x.split(),blank.split('_')))\n",
    "\n",
    "    if(len(blank[0])==2):\n",
    "        all += tri_predict_right(tuple(blank[0]))\n",
    "    if(len(blank[0]) > 0 and len(blank[1]) > 0):\n",
    "        all += tri_predict_mid((blank[0][-1],blank[1][0]))\n",
    "    if(len(blank[1])==2):\n",
    "        all += tri_predict_right(tuple(blank[1]))\n",
    "\n",
    "    all.sort(key=lambda x: x[1], reverse=True)\n",
    "    return all[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all houses were ____ ventilated\n",
      "\n",
      "('made', 1.3778844403226307e-07)\n",
      "('built', 1.3778844403226307e-07)\n",
      "('ventilated', 9.185906816415932e-08)\n",
      "('investigated', 9.185896268817538e-08)\n",
      "('malaria', 9.185896268817538e-08)\n",
      "('contacted', 9.185896268817538e-08)\n",
      "('tested', 9.185896268817538e-08)\n",
      "('then', 9.185896268817538e-08)\n",
      "('no', 9.185896268817538e-08)\n"
     ]
    }
   ],
   "source": [
    "# first sentence\n",
    "print(\"all houses were ____ ventilated\\n\", *tri_predict(\"houses were _ ventilated\"), sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'made ventilated' works, but the prediction of bigrams of 'not ventilated' is a better completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ". . . develop an integrated ____ to reach\n",
      "\n",
      "('the', 7.727704527842534e-05)\n",
      "('a', 4.444692800327732e-05)\n",
      "('their', 7.897594645210433e-06)\n",
      "('an', 7.576180909649543e-06)\n",
      "('approach', 7.394074599874934e-06)\n",
      "('out', 6.290525967405984e-06)\n",
      "('.', 5.509949752472395e-06)\n",
      "('this', 5.326284760723315e-06)\n",
      "('its', 5.1885360169115054e-06)\n"
     ]
    }
   ],
   "source": [
    "# second sentence\n",
    "print(\"\\n\\n. . . develop an integrated ____ to reach\\n\", *tri_predict(\"an integrated _ to reach\"), sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trigram model predicts 'approach' as a possible word, which is better than any suggestions of the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ". . . diagnosis and treatment ____ by involving\n",
      "\n",
      "('of', 0.00018251225083646476)\n",
      "('.', 4.980527971769725e-05)\n",
      "('with', 2.157463729706701e-05)\n",
      "('for', 1.2348037091300056e-05)\n",
      "('and', 1.1521774386305999e-05)\n",
      "('strategies', 9.685635041874764e-06)\n",
      "('in', 9.593828074653203e-06)\n",
      "('is', 9.042986271323833e-06)\n",
      "('are', 7.84949569744353e-06)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n. . . diagnosis and treatment ____ by involving\\n\", *tri_predict(\"and treatment _ by involving\"), sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the models make some possible suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "involving non-health ____ stakeholders from . . .\n",
      "\n",
      "('the', 1.1941608108388649e-06)\n",
      "('different', 5.970804054194324e-07)\n",
      "('across', 5.511511434640915e-07)\n",
      "('a', 3.674340956427277e-07)\n",
      "('multiple', 3.674340956427277e-07)\n",
      "('industry', 3.215048336873867e-07)\n",
      "('diverse', 2.7557557173204575e-07)\n",
      "('government', 2.2964630977670478e-07)\n",
      "('health', 1.8371704782136384e-07)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\ninvolving non-health ____ stakeholders from . . .\\n\", *tri_predict(\"involving non-health _ stakeholders from\"), sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, industry is a possible guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "this is because engineers do not work in ____ but rather as a team\n",
      "\n",
      "('the', 4.403638314445215e-05)\n",
      "('this', 1.565840109724524e-05)\n",
      "('to', 1.5200334205293694e-05)\n",
      "('a', 1.478593886602043e-05)\n",
      "('a', 1.3684893030747797e-05)\n",
      "('the', 1.0286631002978209e-05)\n",
      "('as', 5.418850260497449e-06)\n",
      "('ensuring', 4.959258998541013e-06)\n",
      "('that', 4.500401063802966e-06)\n"
     ]
    }
   ],
   "source": [
    "# third sentence\n",
    "print(\"\\n\\nthis is because engineers do not work in ____ but rather as a team\\n\", *tri_predict(\"work in _ but rather\"), sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has no good guesses, same as the bigram model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import prod\n",
    "# for product of lists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a sentence, calculates the perplexity using bigram model\n",
    "def bi_perplexity(sent):\n",
    "    bigrams = list(ngrams(word_tokenize(sent), 2,\n",
    "                                    pad_left=True, pad_right=True,\n",
    "                                    left_pad_symbol='<s>',\n",
    "                                    right_pad_symbol='</s>'))\n",
    "    return prod([1/bi_prob(tuple(\n",
    "        map(lambda x: invert_vocab[x] if x in vocab else -1, bg)) # creates an array of the bigram reciprocal probabilities of each of the bigram\n",
    "        ) for bg in bigrams]) ** (1/len(bigrams))                 # multiples them all, and then raises it to the power 1/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplecities using bigram model are:\n",
      "462.72741438050633\n",
      "3273.700745887551\n",
      "224632.82214701292\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"it appears that the overall code stroke volume has decreased since the covid- pandemic.\"\n",
    "sent2 = \"half a century ago hypertension was not treatable.\"\n",
    "sent3 = \"sarahs tv is broadcasting an advert for private healthcare\"\n",
    "bi_per = [bi_perplexity(sent) for sent in [sent1, sent2, sent3]]\n",
    "print(\"perplecities using bigram model are:\", *bi_per, sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexitis increase from sentence 1 to sentence 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a sentence, calculates the perplexity using trigram model\n",
    "# function same as of bigram\n",
    "def tri_perplexity(sent):\n",
    "    trigrams = list(ngrams(word_tokenize(sent), 3,\n",
    "                                    pad_left=True, pad_right=True,\n",
    "                                    left_pad_symbol='<s>',\n",
    "                                    right_pad_symbol='</s>'))\n",
    "    return prod([1/tri_prob(tuple(\n",
    "        map(lambda x: invert_vocab[x] if x in vocab else -1, bg))\n",
    "        ) for bg in trigrams]) ** (1/len(trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplecities using bigram model are:\n",
      "52677.135507587234\n",
      "374582.0995995889\n",
      "12675616.670506928\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"it appears that the overall code stroke volume has decreased since the covid- pandemic.\"\n",
    "sent2 = \"half a century ago hypertension was not treatable.\"\n",
    "sent3 = \"sarahs tv is broadcasting an advert for private healthcare\"\n",
    "tri_per = [tri_perplexity(sent) for sent in [sent1, sent2, sent3]]\n",
    "print(\"perplecities using bigram model are:\", *tri_per, sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexities increase, and are also greater than those of the trigram model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
